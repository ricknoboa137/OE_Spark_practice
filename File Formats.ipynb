{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5cce956",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "import random\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "sc = pyspark.SparkContext(appName=\"File_Formats\")\n",
    "spark = SparkSession(sc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363de6b9",
   "metadata": {},
   "source": [
    "# File formats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e3e88c",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "PySpark supports the following file formats:\n",
    "\n",
    "__1. Text files__: PySpark allows you to read and write text files (e.g. CSV, TSV, JSON), which can be read using the SparkContext textFile() method.\n",
    "\n",
    "__2. Sequence files__: A Sequence File is a binary format file containing key-value pairs. PySpark also allows reading and writing Sequence Files.\n",
    "\n",
    "__3. Avro__: Avro is a serialized data format that also supports key-value pairs. PySpark also supports reading and writing Avro files.\n",
    "\n",
    "__4. Parquet__: Parquet is a binary file format that specializes in column-based data storage. PySpark supports both reading and writing Parquet files.\n",
    "\n",
    "__5. ORC__: ORC (Optimized Row Columnar) is another column-based data storage format. PySpark supports both reading and writing ORC files.\n",
    "\n",
    "__6. Hadoop InputFormat__: PySpark also supports Hadoop InputFormat, which allows users to write their own input files for their desired file formats.\n",
    "\n",
    "\n",
    "\n",
    "In addition, PySpark supports all Hadoop supported file formats such as HBase, Cassandra, MongoDB, etc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a349b8c",
   "metadata": {},
   "source": [
    "## 1. Példafeladatok"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c82ba1",
   "metadata": {},
   "source": [
    "### CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9378eeb9",
   "metadata": {},
   "source": [
    "### Parameters to use when scanning:\n",
    "    \n",
    "\n",
    "path: path or pathname of the file(s) to be read. This can be the path to a single file or to an entire directory where the files are located.\n",
    "\n",
    "sep or delimiter: The field delimiter character. The default value is a comma (,), but other characters can be used, such as a tab (\\t).\n",
    "\n",
    "header: Indicates whether the first line of the CSV file contains the column names. Default value is false.\n",
    "\n",
    "inferSchema: Indicates whether Spark automatically guesses the field type. Default value is false.\n",
    "\n",
    "quote: The character used by the fields in the CSV file that are delimited by quotation marks. Defaults to double quotes (\"), but other characters can be used.\n",
    "\n",
    "escape: The escape character to be used before quotation marks or characters with escape characters. The default is double quotes (\"), but other characters may be used.\n",
    "\n",
    "encoding: The character encoding of the file. The default is UTF-8, but other encodings may be used, for example ISO-8859-1.\n",
    "\n",
    "comment: The character used for comments in the CSV file. The default value is null, which means that there is no comment.\n",
    "\n",
    "nullValue: The string represented by the 'null' values used in the file. Default value is null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db1ed7cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'sh'\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "echo \"%%sh - to indicate when you would run a shell script\"\n",
    "\n",
    "# Nem szükséges könyvtárak, fájlok törlése\n",
    "\n",
    "rm -rf /home/student/random_csv\n",
    "rm -rf /home/student/titanic_append_csv\n",
    "rm -rf /home/student/titanic_parquet/\n",
    "rm -rf /home/student/titanic_parquet_append/\n",
    "\n",
    "echo\n",
    "echo \"A fájlformátumok anyag miatt esetlegesen már létrehozott teszt fájlok és mappák törölve\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b25b07b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|        _c0|     _c1|   _c2|                 _c3|   _c4| _c5|  _c6|  _c7|             _c8|    _c9| _c10|    _c11|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|  22|    1|    0|       A/5 21171|   7.25| null|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|  38|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|  26|    0|    0|STON/O2. 3101282|  7.925| null|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|  35|    1|    0|          113803|   53.1| C123|       S|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|  35|    0|    0|          373450|   8.05| null|       S|\n",
      "|          6|       0|     3|    Moran, Mr. James|  male|null|    0|    0|          330877| 8.4583| null|       Q|\n",
      "|          7|       0|     1|McCarthy, Mr. Tim...|  male|  54|    0|    0|           17463|51.8625|  E46|       S|\n",
      "|          8|       0|     3|Palsson, Master. ...|  male|   2|    3|    1|          349909| 21.075| null|       S|\n",
      "|          9|       1|     3|Johnson, Mrs. Osc...|female|  27|    0|    2|          347742|11.1333| null|       S|\n",
      "|         10|       1|     2|Nasser, Mrs. Nich...|female|  14|    1|    0|          237736|30.0708| null|       C|\n",
      "|         11|       1|     3|Sandstrom, Miss. ...|female|   4|    1|    1|         PP 9549|   16.7|   G6|       S|\n",
      "|         12|       1|     1|Bonnell, Miss. El...|female|  58|    0|    0|          113783|  26.55| C103|       S|\n",
      "|         13|       0|     3|Saundercock, Mr. ...|  male|  20|    0|    0|       A/5. 2151|   8.05| null|       S|\n",
      "|         14|       0|     3|Andersson, Mr. An...|  male|  39|    1|    5|          347082| 31.275| null|       S|\n",
      "|         15|       0|     3|Vestrom, Miss. Hu...|female|  14|    0|    0|          350406| 7.8542| null|       S|\n",
      "|         16|       1|     2|Hewlett, Mrs. (Ma...|female|  55|    0|    0|          248706|     16| null|       S|\n",
      "|         17|       0|     3|Rice, Master. Eugene|  male|   2|    4|    1|          382652| 29.125| null|       Q|\n",
      "|         18|       1|     2|Williams, Mr. Cha...|  male|null|    0|    0|          244373|     13| null|       S|\n",
      "|         19|       0|     3|Vander Planke, Mr...|female|  31|    1|    0|          345763|     18| null|       S|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      " |-- _c7: string (nullable = true)\n",
      " |-- _c8: string (nullable = true)\n",
      " |-- _c9: string (nullable = true)\n",
      " |-- _c10: string (nullable = true)\n",
      " |-- _c11: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Scanning without parameterisation\n",
    "\n",
    "\n",
    "# Let's read the CSV file\n",
    "csv_file = spark.read.format(\"csv\").load(\"titanic.csv\")\n",
    "\n",
    "csv_file.show()\n",
    "csv_file.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "491d1738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|  22|    1|    0|       A/5 21171|   7.25| null|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|  38|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|  26|    0|    0|STON/O2. 3101282|  7.925| null|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|  35|    1|    0|          113803|   53.1| C123|       S|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|  35|    0|    0|          373450|   8.05| null|       S|\n",
      "|          6|       0|     3|    Moran, Mr. James|  male|null|    0|    0|          330877| 8.4583| null|       Q|\n",
      "|          7|       0|     1|McCarthy, Mr. Tim...|  male|  54|    0|    0|           17463|51.8625|  E46|       S|\n",
      "|          8|       0|     3|Palsson, Master. ...|  male|   2|    3|    1|          349909| 21.075| null|       S|\n",
      "|          9|       1|     3|Johnson, Mrs. Osc...|female|  27|    0|    2|          347742|11.1333| null|       S|\n",
      "|         10|       1|     2|Nasser, Mrs. Nich...|female|  14|    1|    0|          237736|30.0708| null|       C|\n",
      "|         11|       1|     3|Sandstrom, Miss. ...|female|   4|    1|    1|         PP 9549|   16.7|   G6|       S|\n",
      "|         12|       1|     1|Bonnell, Miss. El...|female|  58|    0|    0|          113783|  26.55| C103|       S|\n",
      "|         13|       0|     3|Saundercock, Mr. ...|  male|  20|    0|    0|       A/5. 2151|   8.05| null|       S|\n",
      "|         14|       0|     3|Andersson, Mr. An...|  male|  39|    1|    5|          347082| 31.275| null|       S|\n",
      "|         15|       0|     3|Vestrom, Miss. Hu...|female|  14|    0|    0|          350406| 7.8542| null|       S|\n",
      "|         16|       1|     2|Hewlett, Mrs. (Ma...|female|  55|    0|    0|          248706|     16| null|       S|\n",
      "|         17|       0|     3|Rice, Master. Eugene|  male|   2|    4|    1|          382652| 29.125| null|       Q|\n",
      "|         18|       1|     2|Williams, Mr. Cha...|  male|null|    0|    0|          244373|     13| null|       S|\n",
      "|         19|       0|     3|Vander Planke, Mr...|female|  31|    1|    0|          345763|     18| null|       S|\n",
      "|         20|       1|     3|Masselmani, Mrs. ...|female|null|    0|    0|            2649|  7.225| null|       C|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- PassengerId: string (nullable = true)\n",
      " |-- Survived: string (nullable = true)\n",
      " |-- Pclass: string (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      " |-- SibSp: string (nullable = true)\n",
      " |-- Parch: string (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: string (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Scanning using a header\n",
    "\n",
    "\n",
    "# Let's read the CSV file\n",
    "csv_file = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"titanic.csv\")\n",
    "\n",
    "csv_file.show()\n",
    "csv_file.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f507fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| null|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| null|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       S|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05| null|       S|\n",
      "|          6|       0|     3|    Moran, Mr. James|  male|null|    0|    0|          330877| 8.4583| null|       Q|\n",
      "|          7|       0|     1|McCarthy, Mr. Tim...|  male|54.0|    0|    0|           17463|51.8625|  E46|       S|\n",
      "|          8|       0|     3|Palsson, Master. ...|  male| 2.0|    3|    1|          349909| 21.075| null|       S|\n",
      "|          9|       1|     3|Johnson, Mrs. Osc...|female|27.0|    0|    2|          347742|11.1333| null|       S|\n",
      "|         10|       1|     2|Nasser, Mrs. Nich...|female|14.0|    1|    0|          237736|30.0708| null|       C|\n",
      "|         11|       1|     3|Sandstrom, Miss. ...|female| 4.0|    1|    1|         PP 9549|   16.7|   G6|       S|\n",
      "|         12|       1|     1|Bonnell, Miss. El...|female|58.0|    0|    0|          113783|  26.55| C103|       S|\n",
      "|         13|       0|     3|Saundercock, Mr. ...|  male|20.0|    0|    0|       A/5. 2151|   8.05| null|       S|\n",
      "|         14|       0|     3|Andersson, Mr. An...|  male|39.0|    1|    5|          347082| 31.275| null|       S|\n",
      "|         15|       0|     3|Vestrom, Miss. Hu...|female|14.0|    0|    0|          350406| 7.8542| null|       S|\n",
      "|         16|       1|     2|Hewlett, Mrs. (Ma...|female|55.0|    0|    0|          248706|   16.0| null|       S|\n",
      "|         17|       0|     3|Rice, Master. Eugene|  male| 2.0|    4|    1|          382652| 29.125| null|       Q|\n",
      "|         18|       1|     2|Williams, Mr. Cha...|  male|null|    0|    0|          244373|   13.0| null|       S|\n",
      "|         19|       0|     3|Vander Planke, Mr...|female|31.0|    1|    0|          345763|   18.0| null|       S|\n",
      "|         20|       1|     3|Masselmani, Mrs. ...|female|null|    0|    0|            2649|  7.225| null|       C|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Scanning using header and inferschema\n",
    "\n",
    "\n",
    "# Let's read the CSV file\n",
    "csv_file_with_schema = spark.read.format(\"csv\") \\\n",
    ".option(\"header\", \"true\") \\\n",
    ".option(\"inferSchema\", \"true\") \\\n",
    ".load(\"titanic.csv\")\n",
    "\n",
    "csv_file_with_schema.show()\n",
    "csv_file_with_schema.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "272cda8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                 _c0|\n",
      "+--------------------+\n",
      "|PassengerId,Survi...|\n",
      "|1,0,3,\"Braund, Mr...|\n",
      "|2,1,1,\"Cumings, M...|\n",
      "|3,1,3,\"Heikkinen,...|\n",
      "|4,1,1,\"Futrelle, ...|\n",
      "|5,0,3,\"Allen, Mr....|\n",
      "|6,0,3,\"Moran, Mr....|\n",
      "|7,0,1,\"McCarthy, ...|\n",
      "|8,0,3,\"Palsson, M...|\n",
      "|9,1,3,\"Johnson, M...|\n",
      "|10,1,2,\"Nasser, M...|\n",
      "|11,1,3,\"Sandstrom...|\n",
      "|12,1,1,\"Bonnell, ...|\n",
      "|13,0,3,\"Saunderco...|\n",
      "|14,0,3,\"Andersson...|\n",
      "|15,0,3,\"Vestrom, ...|\n",
      "|16,1,2,\"Hewlett, ...|\n",
      "|17,0,3,\"Rice, Mas...|\n",
      "|18,1,2,\"Williams,...|\n",
      "|19,0,3,\"Vander Pl...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Scan using another delimiter\n",
    "\n",
    "\n",
    "# Let's read the CSV file\n",
    "csv_file = spark.read.format(\"csv\").option(\"delimiter\", \";\").load(\"titanic.csv\")\n",
    "\n",
    "csv_file.show()\n",
    "csv_file.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e53f2d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_ALREADY_EXISTS] Path file:/C:/Users/erick/Documents/Doctoral School/DataBases 2023/Jupyter spark/random_csv already exists. Set mode as \"overwrite\" to overwrite the existing path.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_28256\\2618765716.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"value\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moption\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"header\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"true\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"random_csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\spark-3.4.0-bin-hadoop3\\python\\pyspark\\sql\\readwriter.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[0;32m   1396\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1397\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1398\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1399\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1400\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minsertInto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtableName\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-3.4.0-bin-hadoop3\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1324\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-3.4.0-bin-hadoop3\\python\\pyspark\\errors\\exceptions\\captured.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    173\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    176\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: [PATH_ALREADY_EXISTS] Path file:/C:/Users/erick/Documents/Doctoral School/DataBases 2023/Jupyter spark/random_csv already exists. Set mode as \"overwrite\" to overwrite the existing path."
     ]
    }
   ],
   "source": [
    "# Generate random row and write a new CSV file\n",
    "\n",
    "from pyspark.sql.functions import rand\n",
    "\n",
    "# DataFrame of 10 rows with random numbers\n",
    "df = spark.range(0, 10).withColumn(\"value\", rand())\n",
    "\n",
    "df.write.format(\"csv\").option(\"header\", \"true\").save(\"random_csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06d734f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random row generation and modify CSV file by rewriting\n",
    "\n",
    "from pyspark.sql.functions import rand\n",
    "\n",
    "# DataFrame of 10 rows with random numbers\n",
    "df = spark.range(0, 10).withColumn(\"value\", rand())\n",
    "\n",
    "df.write.format(\"csv\").option(\"header\", \"true\").mode('overwrite').save(\"random_csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aef2ceaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random row and modify CSV file with append\n",
    "\n",
    "from pyspark.sql.functions import rand\n",
    "\n",
    "# DataFrame of 10 rows with random numbers\n",
    "df = spark.range(0, 10).withColumn(\"value\", rand())\n",
    "\n",
    "df.write.format(\"csv\").option(\"header\", \"true\").mode('append').save(\"random_csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2fe91fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scan existing data and then write a file with append\n",
    "\n",
    "# load a data frame from file\n",
    "df = spark.read.format(\"csv\").load(\"titanic.csv\")\n",
    "\n",
    "df.write.format(\"csv\").option(\"header\", \"true\").mode('append').save(\"titanic_append_csv\")\n",
    "\n",
    "# Once you've run it a few times, it's worth checking the contents in the file manager\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d2f969",
   "metadata": {},
   "source": [
    "### JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c72dbd39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+--------+-------+------------------+--------------+-----------+--------------------+------+------------------+--------+----------------+\n",
      "|Age|Cabin|Embarked|   Fare|ParentsAndChildren|PassengerClass|PassengerId|       PassengerName|   Sex|SiblingsAndSpouses|Survived|          Ticket|\n",
      "+---+-----+--------+-------+------------------+--------------+-----------+--------------------+------+------------------+--------+----------------+\n",
      "| 22|     |       S|   7.25|                 0|             3|          1|Braund, Mr. Owen ...|  male|                 1|       0|       A/5 21171|\n",
      "| 38|  C85|       C|71.2833|                 0|             1|          2|Cumings, Mrs. Joh...|female|                 1|       1|        PC 17599|\n",
      "| 26|     |       S|  7.925|                 0|             3|          3|Heikkinen, Miss. ...|female|                 0|       1|STON/O2. 3101282|\n",
      "| 35| C123|       S|   53.1|                 0|             1|          4|Futrelle, Mrs. Ja...|female|                 1|       1|          113803|\n",
      "| 35|     |       S|   8.05|                 0|             3|          5|Allen, Mr. Willia...|  male|                 0|       0|          373450|\n",
      "|   |     |       Q| 8.4583|                 0|             3|          6|    Moran, Mr. James|  male|                 0|       0|          330877|\n",
      "| 54|  E46|       S|51.8625|                 0|             1|          7|McCarthy, Mr. Tim...|  male|                 0|       0|           17463|\n",
      "|  2|     |       S| 21.075|                 1|             3|          8|Palsson, Master. ...|  male|                 3|       0|          349909|\n",
      "| 27|     |       S|11.1333|                 2|             3|          9|Johnson, Mrs. Osc...|female|                 0|       1|          347742|\n",
      "| 14|     |       C|30.0708|                 0|             2|         10|Nasser, Mrs. Nich...|female|                 1|       1|          237736|\n",
      "|  4|   G6|       S|   16.7|                 1|             3|         11|Sandstrom, Miss. ...|female|                 1|       1|         PP 9549|\n",
      "| 58| C103|       S|  26.55|                 0|             1|         12|Bonnell, Miss. El...|female|                 0|       1|          113783|\n",
      "| 20|     |       S|   8.05|                 0|             3|         13|Saundercock, Mr. ...|  male|                 0|       0|       A/5. 2151|\n",
      "| 39|     |       S| 31.275|                 5|             3|         14|Andersson, Mr. An...|  male|                 1|       0|          347082|\n",
      "| 14|     |       S| 7.8542|                 0|             3|         15|Vestrom, Miss. Hu...|female|                 0|       0|          350406|\n",
      "| 55|     |       S|     16|                 0|             2|         16|Hewlett, Mrs. (Ma...|female|                 0|       1|          248706|\n",
      "|  2|     |       Q| 29.125|                 1|             3|         17|Rice, Master. Eugene|  male|                 4|       0|          382652|\n",
      "|   |     |       S|     13|                 0|             2|         18|Williams, Mr. Cha...|  male|                 0|       1|          244373|\n",
      "| 31|     |       S|     18|                 0|             3|         19|Vander Planke, Mr...|female|                 1|       0|          345763|\n",
      "|   |     |       C|  7.225|                 0|             3|         20|Masselmani, Mrs. ...|female|                 0|       1|            2649|\n",
      "+---+-----+--------+-------+------------------+--------------+-----------+--------------------+------+------------------+--------+----------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- Age: string (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      " |-- Fare: string (nullable = true)\n",
      " |-- ParentsAndChildren: string (nullable = true)\n",
      " |-- PassengerClass: string (nullable = true)\n",
      " |-- PassengerId: string (nullable = true)\n",
      " |-- PassengerName: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- SiblingsAndSpouses: string (nullable = true)\n",
      " |-- Survived: string (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Scanning \n",
    "\n",
    "\n",
    "# Let's read the JSON file\n",
    "json_file = spark.read.format(\"json\").load(\"titanic.json\")\n",
    "json_file.show()\n",
    "json_file.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5034854f",
   "metadata": {},
   "source": [
    "### Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b53a3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kiiratás\n",
    "# Write the dataframe from the CSV to Parquet format\n",
    "\n",
    "csv_file_with_schema.write.format(\"parquet\").save(\"titanic_parquet/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00b6d136",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_ALREADY_EXISTS] Path file:/C:/Users/erick/Documents/Doctoral School/DataBases 2023/Jupyter spark/titanic_parquet already exists. Set mode as \"overwrite\" to overwrite the existing path.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20396\\662026585.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Futtassuk le mégegyszer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mcsv_file_with_schema\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"parquet\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"titanic_parquet/\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\spark-3.4.0-bin-hadoop3\\python\\pyspark\\sql\\readwriter.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[0;32m   1396\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1397\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1398\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1399\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1400\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minsertInto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtableName\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-3.4.0-bin-hadoop3\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1324\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark-3.4.0-bin-hadoop3\\python\\pyspark\\errors\\exceptions\\captured.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    173\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    176\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: [PATH_ALREADY_EXISTS] Path file:/C:/Users/erick/Documents/Doctoral School/DataBases 2023/Jupyter spark/titanic_parquet already exists. Set mode as \"overwrite\" to overwrite the existing path."
     ]
    }
   ],
   "source": [
    "# Let's run it again\n",
    "\n",
    "csv_file_with_schema.write.format(\"parquet\").save(\"titanic_parquet/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82eed79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# So we can overwrite\n",
    "\n",
    "csv_file_with_schema.write.format(\"parquet\").mode(\"overwrite\").save(\"titanic_parquet/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9573ba68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to add the data\n",
    "\n",
    "csv_file_with_schema.write.format(\"parquet\").mode(\"append\").save(\"titanic_parquet_append/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "55a2d0c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| null|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| null|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       S|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05| null|       S|\n",
      "|          6|       0|     3|    Moran, Mr. James|  male|null|    0|    0|          330877| 8.4583| null|       Q|\n",
      "|          7|       0|     1|McCarthy, Mr. Tim...|  male|54.0|    0|    0|           17463|51.8625|  E46|       S|\n",
      "|          8|       0|     3|Palsson, Master. ...|  male| 2.0|    3|    1|          349909| 21.075| null|       S|\n",
      "|          9|       1|     3|Johnson, Mrs. Osc...|female|27.0|    0|    2|          347742|11.1333| null|       S|\n",
      "|         10|       1|     2|Nasser, Mrs. Nich...|female|14.0|    1|    0|          237736|30.0708| null|       C|\n",
      "|         11|       1|     3|Sandstrom, Miss. ...|female| 4.0|    1|    1|         PP 9549|   16.7|   G6|       S|\n",
      "|         12|       1|     1|Bonnell, Miss. El...|female|58.0|    0|    0|          113783|  26.55| C103|       S|\n",
      "|         13|       0|     3|Saundercock, Mr. ...|  male|20.0|    0|    0|       A/5. 2151|   8.05| null|       S|\n",
      "|         14|       0|     3|Andersson, Mr. An...|  male|39.0|    1|    5|          347082| 31.275| null|       S|\n",
      "|         15|       0|     3|Vestrom, Miss. Hu...|female|14.0|    0|    0|          350406| 7.8542| null|       S|\n",
      "|         16|       1|     2|Hewlett, Mrs. (Ma...|female|55.0|    0|    0|          248706|   16.0| null|       S|\n",
      "|         17|       0|     3|Rice, Master. Eugene|  male| 2.0|    4|    1|          382652| 29.125| null|       Q|\n",
      "|         18|       1|     2|Williams, Mr. Cha...|  male|null|    0|    0|          244373|   13.0| null|       S|\n",
      "|         19|       0|     3|Vander Planke, Mr...|female|31.0|    1|    0|          345763|   18.0| null|       S|\n",
      "|         20|       1|     3|Masselmani, Mrs. ...|female|null|    0|    0|            2649|  7.225| null|       C|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's scan the data\n",
    "\n",
    "df_parquet = spark.read.format(\"parquet\").load(\"titanic_parquet\")\n",
    "\n",
    "df_parquet.show()\n",
    "df_parquet.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef516e3",
   "metadata": {},
   "source": [
    "## 2. Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038f3f61",
   "metadata": {},
   "source": [
    "Extract the contents of the /home/student directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b696779",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Cell magic `%%` not found.\n"
     ]
    }
   ],
   "source": [
    "%%\n",
    "\n",
    "ls /home/student"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a46f59",
   "metadata": {},
   "source": [
    "### CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "02706145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|        _c0|     _c1|   _c2|                 _c3|   _c4| _c5|  _c6|  _c7|             _c8|    _c9| _c10|    _c11|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|  22|    1|    0|       A/5 21171|   7.25| null|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|  38|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|  26|    0|    0|STON/O2. 3101282|  7.925| null|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|  35|    1|    0|          113803|   53.1| C123|       S|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|  35|    0|    0|          373450|   8.05| null|       S|\n",
      "|          6|       0|     3|    Moran, Mr. James|  male|null|    0|    0|          330877| 8.4583| null|       Q|\n",
      "|          7|       0|     1|McCarthy, Mr. Tim...|  male|  54|    0|    0|           17463|51.8625|  E46|       S|\n",
      "|          8|       0|     3|Palsson, Master. ...|  male|   2|    3|    1|          349909| 21.075| null|       S|\n",
      "|          9|       1|     3|Johnson, Mrs. Osc...|female|  27|    0|    2|          347742|11.1333| null|       S|\n",
      "|         10|       1|     2|Nasser, Mrs. Nich...|female|  14|    1|    0|          237736|30.0708| null|       C|\n",
      "|         11|       1|     3|Sandstrom, Miss. ...|female|   4|    1|    1|         PP 9549|   16.7|   G6|       S|\n",
      "|         12|       1|     1|Bonnell, Miss. El...|female|  58|    0|    0|          113783|  26.55| C103|       S|\n",
      "|         13|       0|     3|Saundercock, Mr. ...|  male|  20|    0|    0|       A/5. 2151|   8.05| null|       S|\n",
      "|         14|       0|     3|Andersson, Mr. An...|  male|  39|    1|    5|          347082| 31.275| null|       S|\n",
      "|         15|       0|     3|Vestrom, Miss. Hu...|female|  14|    0|    0|          350406| 7.8542| null|       S|\n",
      "|         16|       1|     2|Hewlett, Mrs. (Ma...|female|  55|    0|    0|          248706|     16| null|       S|\n",
      "|         17|       0|     3|Rice, Master. Eugene|  male|   2|    4|    1|          382652| 29.125| null|       Q|\n",
      "|         18|       1|     2|Williams, Mr. Cha...|  male|null|    0|    0|          244373|     13| null|       S|\n",
      "|         19|       0|     3|Vander Planke, Mr...|female|  31|    1|    0|          345763|     18| null|       S|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      " |-- _c7: string (nullable = true)\n",
      " |-- _c8: string (nullable = true)\n",
      " |-- _c9: string (nullable = true)\n",
      " |-- _c10: string (nullable = true)\n",
      " |-- _c11: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read the CSV file and display its contents 1.\n",
    "csv_file = spark.read..load(\"titanic.csv\")\n",
    "\n",
    "csv_file.show()\n",
    "csv_file.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9876d869",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1198416730.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\erick\\AppData\\Local\\Temp\\ipykernel_20396\\1198416730.py\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    csv_file =\u001b[0m\n\u001b[1;37m               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Read the CSV file and display its contents 2.\n",
    "csv_file = \n",
    "csv_file.show()\n",
    "csv_file.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36f2d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file and display its contents 1. \n",
    "\n",
    "csv_file = spark.read.format(\"csv\").load(\"titanic.csv\")\n",
    "\n",
    "csv_file.show()\n",
    "csv_file.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1b433da7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| null|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| null|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       S|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05| null|       S|\n",
      "|          6|       0|     3|    Moran, Mr. James|  male|null|    0|    0|          330877| 8.4583| null|       Q|\n",
      "|          7|       0|     1|McCarthy, Mr. Tim...|  male|54.0|    0|    0|           17463|51.8625|  E46|       S|\n",
      "|          8|       0|     3|Palsson, Master. ...|  male| 2.0|    3|    1|          349909| 21.075| null|       S|\n",
      "|          9|       1|     3|Johnson, Mrs. Osc...|female|27.0|    0|    2|          347742|11.1333| null|       S|\n",
      "|         10|       1|     2|Nasser, Mrs. Nich...|female|14.0|    1|    0|          237736|30.0708| null|       C|\n",
      "|         11|       1|     3|Sandstrom, Miss. ...|female| 4.0|    1|    1|         PP 9549|   16.7|   G6|       S|\n",
      "|         12|       1|     1|Bonnell, Miss. El...|female|58.0|    0|    0|          113783|  26.55| C103|       S|\n",
      "|         13|       0|     3|Saundercock, Mr. ...|  male|20.0|    0|    0|       A/5. 2151|   8.05| null|       S|\n",
      "|         14|       0|     3|Andersson, Mr. An...|  male|39.0|    1|    5|          347082| 31.275| null|       S|\n",
      "|         15|       0|     3|Vestrom, Miss. Hu...|female|14.0|    0|    0|          350406| 7.8542| null|       S|\n",
      "|         16|       1|     2|Hewlett, Mrs. (Ma...|female|55.0|    0|    0|          248706|   16.0| null|       S|\n",
      "|         17|       0|     3|Rice, Master. Eugene|  male| 2.0|    4|    1|          382652| 29.125| null|       Q|\n",
      "|         18|       1|     2|Williams, Mr. Cha...|  male|null|    0|    0|          244373|   13.0| null|       S|\n",
      "|         19|       0|     3|Vander Planke, Mr...|female|31.0|    1|    0|          345763|   18.0| null|       S|\n",
      "|         20|       1|     3|Masselmani, Mrs. ...|female|null|    0|    0|            2649|  7.225| null|       C|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read the CSV file and display its contents: debug exercise 2: \n",
    "csv_file = spark.read.format(\"parquet\").load(\"titanic_parquet\")\n",
    "\n",
    "csv_file.show()\n",
    "csv_file.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1649729f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Olvassuk be a CSV fájlt, és jelenítsük meg a tartalmát: Hibakeresés feladat 3: \n",
    "csv_file = spark.read.format(\"csv\").save(\"titanic.csv\")\n",
    "\n",
    "csv_file.show()\n",
    "csv_file.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8d5b05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scanning using header and schema identification 1.\n",
    "\n",
    "# Read the CSV file\n",
    "csv_file_with_schema = spark.read.format(\"csv\") \\\n",
    ".option(\"\", \"true\") \\\n",
    ".option(\"\", \"true\") \\\n",
    ".load(\"titanic.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d535c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scanning using header and schema identification 2.\n",
    "\n",
    "\n",
    "# Read the CSV file\n",
    "csv_file_with_schema = spark.read.format(\"csv\") \\\n",
    "\n",
    "\n",
    ".load(\"titanic.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89231ff3",
   "metadata": {},
   "source": [
    "### JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a325efa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's read the JSON file. 1.\n",
    "json_file = spark.read.format(\"\").load(\"/home/student/titanic.json\")\n",
    "json_file.show()\n",
    "json_file.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc23e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's read the JSON file. 2.\n",
    "json_file = spark.read..load(\"/home/student/titanic.json\")\n",
    "json_file.show()\n",
    "json_file.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a212a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's read the JSON file. 3.\n",
    "json_file = spark.read.  (\"/home/student/titanic.json\")\n",
    "json_file.show()\n",
    "json_file.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8f2df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's read the JSON file. 4.\n",
    "json_file = spark.   (\"/home/student/titanic.json\")\n",
    "json_file.show()\n",
    "json_file.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e006943",
   "metadata": {},
   "source": [
    "### Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f40b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Parquet file \n",
    "\n",
    "df_parquet = spark.read.format(\"\").load(\"titanic_parquet\")\n",
    "\n",
    "df_parquet.show()\n",
    "df_parquet.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc94f027",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Parquet file\n",
    "\n",
    "df_parquet = spark.read..load(\"titanic_parquet\")\n",
    "\n",
    "df_parquet.show()\n",
    "df_parquet.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef49f3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Parquet file\n",
    "\n",
    "df_parquet = spark..load(\"/home/student/titanic_parquet\")\n",
    "\n",
    "df_parquet.show()\n",
    "df_parquet.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17b126e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Parquet file\n",
    "\n",
    "df_parquet = /home/student/titanic_parquet\n",
    "\n",
    "df_parquet.show()\n",
    "df_parquet.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6278bd5",
   "metadata": {},
   "source": [
    "### Stop SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc35e3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1dd033",
   "metadata": {},
   "source": [
    "## 3. Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2e9c1a",
   "metadata": {},
   "source": [
    "Homework to look up and solve:\n",
    "    \n",
    "1. What other file formats does Spark support? What other other Spark features does Spark support? \n",
    "2. Kiiratáskor definiáljátok a különböző tömörítési eljárásokat.\n",
    "3. what are the other methods of execution besides append and overwrite? Write an example task for this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b086f621",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
